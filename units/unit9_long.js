// Unit 9 – Frequent Pattern Mining 10-mark Questions
window.unit9_long = [
  {
    unit: "Unit 9 – Frequent Pattern Mining",
    question: "Explain the Apriori Algorithm with an example.",
    keyPoints:
      "Apriori is a level-wise algorithm used to find frequent itemsets in transactional databases.\n\n" +
      "Key Concepts:\n" +
      "- Support: proportion of transactions containing an itemset.\n" +
      "- Confidence: strength of a rule (X → Y).\n" +
      "- Apriori property: all non-empty subsets of a frequent itemset must also be frequent.\n\n" +
      "Steps of Apriori:\n" +
      "1) Generate frequent 1-itemsets (L1):\n" +
      "- Scan database and count support for each single item.\n" +
      "- Keep items with support ≥ min_support.\n\n" +
      "2) Generate candidate k-itemsets (Ck) from L(k-1):\n" +
      "- Join frequent itemsets from previous level.\n" +
      "- Use Apriori property to prune candidates whose subsets are not frequent.\n\n" +
      "3) Scan DB to compute support of Ck and keep only those with support ≥ min_support to form Lk.\n\n" +
      "4) Repeat until no new frequent itemsets can be found.\n\n" +
      "Example (simple):\n" +
      "- Transactions T1–T5 with items A, B, C.\n" +
      "- min_support = 2.\n" +
      "- L1: {A}, {B}, {C} all frequent.\n" +
      "- C2: {A,B}, {A,C}, {B,C}.\n" +
      "- L2: all three are frequent (support ≥ 2).\n" +
      "- C3: {A,B,C} has support 1 < 2 → not frequent.\n\n" +
      "Result: Frequent itemsets = {A}, {B}, {C}, {A,B}, {A,C}, {B,C}."
  },
  {
    unit: "Unit 9 – Frequent Pattern Mining",
    question: "Explain FP-Growth algorithm and compare it with Apriori.",
    keyPoints:
      "FP-Growth (Frequent Pattern Growth) is a tree-based frequent pattern mining method that avoids candidate generation.\n\n" +
      "Steps in FP-Growth:\n" +
      "1) First DB Scan:\n" +
      "- Compute frequent items and sort them by descending support.\n\n" +
      "2) Build FP-Tree:\n" +
      "- Second DB scan.\n" +
      "- For each transaction, insert its frequent items (in sorted order) into an FP-tree.\n" +
      "- Common prefixes are merged, counts stored in nodes.\n\n" +
      "3) Mine the FP-Tree:\n" +
      "- For each item, build its conditional pattern base and conditional FP-tree.\n" +
      "- Recursively extract frequent itemsets.\n\n" +
      "Comparison with Apriori:\n" +
      "- DB Scans: Apriori needs many passes; FP-Growth usually needs only 2.\n" +
      "- Candidate Generation: Apriori generates candidate sets; FP-Growth avoids candidates.\n" +
      "- Speed: FP-Growth is generally faster on large dense datasets.\n" +
      "- Memory: Apriori uses less memory but more scans; FP-Growth stores an FP-tree which may be large.\n\n" +
      "Conclusion: FP-Growth is more efficient than Apriori for many practical frequent pattern mining tasks."
  },
  {
    unit: "Unit 9 – Frequent Pattern Mining",
    question: "Explain Support, Confidence and Lift with examples.",
    keyPoints:
      "1) Support:\n" +
      "- Measures how frequently X and Y occur together.\n" +
      "- Support(X → Y) = (Number of transactions containing X and Y) / (Total transactions).\n" +
      "Example: 100 transactions, 20 have both A and B → Support(A → B) = 20/100 = 0.20.\n\n" +
      "2) Confidence:\n" +
      "- Measures how often Y occurs when X occurs.\n" +
      "- Confidence(X → Y) = Support(X ∪ Y) / Support(X).\n" +
      "Example: Support(A) = 40/100, Support(A,B) = 20/100 → Confidence(A → B) = 20/40 = 0.50.\n\n" +
      "3) Lift:\n" +
      "- Measures how much more often X and Y occur together than if they were independent.\n" +
      "- Lift(X → Y) = Confidence(X → Y) / Support(Y).\n" +
      "Example: Support(B) = 50/100 = 0.50, Confidence(A → B) = 0.50 → Lift = 0.50 / 0.50 = 1.\n" +
      "- Lift > 1: positive correlation.\n" +
      "- Lift < 1: negative correlation.\n" +
      "- Lift = 1: X and Y are independent.\n\n" +
      "Conclusion: Support checks frequency, confidence checks rule strength, and lift checks true interestingness."
  },
  {
    unit: "Unit 9 – Frequent Pattern Mining",
    question:
      "What is an Itemset? Explain frequent itemsets, candidate itemsets and closed itemsets.",
    keyPoints:
      "Itemset:\n" +
      "- A set of one or more items, e.g., {Milk, Bread}.\n\n" +
      "Frequent Itemset:\n" +
      "- An itemset whose support ≥ minimum support threshold.\n" +
      "- Used for generating association rules.\n\n" +
      "Candidate Itemset:\n" +
      "- Itemset generated by algorithms like Apriori before checking support.\n" +
      "- Example: C2, C3, etc. are candidate sets that may or may not be frequent.\n\n" +
      "Closed Itemset:\n" +
      "- An itemset is closed if none of its proper supersets have the same support.\n" +
      "- Example: If {A,B} and {A,B,C} both have support 3, then {A,B} is NOT closed; {A,B,C} is closed.\n\n" +
      "Importance:\n" +
      "- Closed itemsets provide a compact but lossless representation of frequent patterns."
  },
  {
    unit: "Unit 9 – Frequent Pattern Mining",
    question: "Explain the steps involved in Association Rule Mining.",
    keyPoints:
      "Association rule mining discovers interesting relationships such as X → Y in transaction data.\n\n" +
      "Typical Steps:\n" +
      "1) Find Frequent Itemsets:\n" +
      "- Use Apriori, FP-Growth or other algorithms.\n" +
      "- Only itemsets with support ≥ min_support are kept.\n\n" +
      "2) Generate Candidate Rules:\n" +
      "- For each frequent itemset L, generate rules of the form X → Y where X ∪ Y = L and X ∩ Y = ∅.\n\n" +
      "3) Compute Measures:\n" +
      "- Support, Confidence, and often Lift for each rule.\n\n" +
      "4) Apply Thresholds:\n" +
      "- Keep only rules with confidence ≥ min_confidence (and support ≥ min_support).\n\n" +
      "5) Evaluate and Sort Rules:\n" +
      "- Rank rules by confidence, lift or other interestingness measures.\n" +
      "- Select top rules for business interpretation.\n\n" +
      "Conclusion: Association rule mining turns frequent itemsets into actionable knowledge (e.g., 'if a customer buys X, they are likely to buy Y')."
  }
];
